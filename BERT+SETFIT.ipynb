{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import Dataset\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to numbers\n",
    "numerize_labels = {'Politics':0, 'Health':1, 'Finance':2, 'Travel':3, 'Food':4, 'Education':5,\n",
    "       'Environment':6, 'Fashion':7, 'Science':8, 'Sports':9, 'Technology':10, 'Entertainment':11}\n",
    "\n",
    "# Load your JSON file\n",
    "with open('./data/train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert the data into the required format\n",
    "formatted_data = []\n",
    "idx=0\n",
    "for label, sentences in data.items():\n",
    "    for  sentence in sentences:\n",
    "        formatted_data.append({'label': numerize_labels[label], 'sentence': sentence, 'idx': idx})\n",
    "        idx+=1\n",
    "\n",
    "# Create a HuggingFace Dataset\n",
    "dataset = Dataset.from_list(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "/var/folders/dw/dqt68cy915l_fd4_jgbffvlr0000gn/T/ipykernel_4019/3489387944.py:3: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02a729806f64647a7ab4805c99606d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 1440\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906109eddcfd4e369f0d6d2d5d1cc763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ff7e458a934c40b842a193575d0188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.1765, 'learning_rate': 2.222222222222222e-06, 'epoch': 0.01}\n",
      "{'embedding_loss': 0.0137, 'learning_rate': 9.876543209876543e-06, 'epoch': 0.56}\n",
      "{'train_runtime': 525.9777, 'train_samples_per_second': 2.738, 'train_steps_per_second': 0.171, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31106a04d7b449c8ae115f2f80f9e187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setfit_model = SetFitModel.from_pretrained('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "\n",
    "trainer = SetFitTrainer(\n",
    "model=setfit_model,\n",
    "train_dataset=dataset,\n",
    "eval_dataset=dataset,\n",
    "loss_class=CosineSimilarityLoss,\n",
    "metric=\"accuracy\",\n",
    "batch_size=16,\n",
    "num_iterations=20,\n",
    "num_epochs=1,\n",
    "column_mapping={\"sentence\": \"text\", \"label\": \"label\"})\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/test_shuffle.txt', 'r') as f:\n",
    "    test = f.readlines()\n",
    "\n",
    "# Remove newline characters\n",
    "test = [line.strip() for line in test]\n",
    "reverse_numerize_labels = {v: k for k, v in numerize_labels.items()}\n",
    "good_preds=[]\n",
    "for sentence in test:\n",
    "    preds=setfit_model.predict_proba(sentence)\n",
    "    if preds[np.argmax(preds)]>0.8:\n",
    "        good_preds.append((sentence, reverse_numerize_labels[int(setfit_model(sentence))]))\n",
    "\n",
    "def count_sentences_per_label(good_preds):\n",
    "    # Initialize a dictionary to store the counts per label\n",
    "    label_counts = {}\n",
    "\n",
    "    # Iterate through the (sentence, label) pairs\n",
    "    for sentence, label in good_preds:\n",
    "        # Check if the label exists in the dictionary\n",
    "        if reverse_numerize_labels[int(label)] in label_counts:\n",
    "            # Increment the count for the label\n",
    "            label_counts[reverse_numerize_labels[int(label)]] += 1\n",
    "        else:\n",
    "            # Initialize the count for the label\n",
    "            label_counts[reverse_numerize_labels[int(label)]] = 1\n",
    "\n",
    "    return label_counts\n",
    "\n",
    "# Example usage:\n",
    "label_counts = count_sentences_per_label(good_preds)\n",
    "print(label_counts)\n",
    "\n",
    "def extract_sentences_per_label(good_preds, n=30):\n",
    "    # Initialize a dictionary to store sentences per label\n",
    "    sentences_per_label = {}\n",
    "\n",
    "    # Iterate through the (sentence, label) pairs\n",
    "    for sentence, label in good_preds:\n",
    "        # Check if the label exists in the dictionary\n",
    "        if int(label) not in sentences_per_label:\n",
    "            sentences_per_label[int(label)] = []\n",
    "\n",
    "        # Append the sentence to the list corresponding to its label\n",
    "        sentences_per_label[int(label)].append(sentence)\n",
    "\n",
    "    # Initialize a list to store exactly 10 sentences per label\n",
    "    extracted_sentences_per_label = []\n",
    "\n",
    "    # Iterate through the sentences per label and extract 10 sentences for each label\n",
    "    for label, sentences in sentences_per_label.items():\n",
    "        # Extend the list with the first 10 sentences for the label\n",
    "        extracted_sentences_per_label.extend([(sentence, int(label)) for sentence in sentences[:n]])\n",
    "\n",
    "    return extracted_sentences_per_label\n",
    "\n",
    "sentences_per_label = extract_sentences_per_label(good_preds, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The mayor announced a new initiative to improv...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The senator is facing criticism for her stance...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The upcoming election has sparked intense deba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The political climate is becoming increasingly...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The opposition party is calling for a referend...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  The mayor announced a new initiative to improv...      0\n",
       "1  The senator is facing criticism for her stance...      0\n",
       "2  The upcoming election has sparked intense deba...      0\n",
       "3  The political climate is becoming increasingly...      0\n",
       "4  The opposition party is calling for a referend...      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"./data/train.json\"\n",
    "\n",
    "with open(DATA_PATH, \"r\") as fichier:\n",
    "    data = json.load(fichier)\n",
    "\n",
    "data_augmented = data.copy()\n",
    "\n",
    "for sentence, label in good_preds:\n",
    "    data_augmented[label].append(sentence)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df = df.stack().reset_index(level=1, drop=True).reset_index()\n",
    "df.columns = ['Label', 'Text']\n",
    "df = df[['Text', 'Label']]\n",
    "\n",
    "numerize_labels = {'Politics':0, 'Health':1, 'Finance':2, 'Travel':3, 'Food':4, 'Education':5,\n",
    "       'Environment':6, 'Fashion':7, 'Science':8, 'Sports':9, 'Technology':10, 'Entertainment':11}\n",
    "\n",
    "df.Label = df.Label.map(numerize_labels)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the dataset into training and validation sets\n",
    "train_dataset = pd.DataFrame(columns=['Text', 'Label'])\n",
    "val_dataset = pd.DataFrame(columns=['Text', 'Label'])\n",
    "\n",
    "for key in numerize_labels:\n",
    "    df_key = df[df['Label'] == numerize_labels[key]]\n",
    "    train_dataset = pd.concat([train_dataset, df_key.sample(frac=0.8)])\n",
    "    val_dataset = pd.concat([val_dataset, df_key.drop(df_key.sample(frac=0.8).index)])\n",
    "        \n",
    "# Step 2: Load the pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(numerize_labels)).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Step 3: Tokenize the text data using the BERT tokenizer\n",
    "train_encodings = tokenizer(list(train_dataset['Text']), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(list(val_dataset['Text']), truncation=True, padding=True)\n",
    "\n",
    "# Step 4: Convert the tokenized data into input features compatible with BERT\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': list(train_dataset['Label'])})\n",
    "val_dataset = Dataset.from_dict({'input_ids': val_encodings['input_ids'], 'attention_mask': val_encodings['attention_mask'], 'labels': list(val_dataset['Label'])})\n",
    "\n",
    "# Step 5: Define the model architecture\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=300,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Step 6: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['ID', 'Label'])\n",
    "\n",
    "file_path = \"./data/test_shuffle.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "for i, sentence in enumerate(text.split(\"\\n\")):\n",
    "    inputs = tokenizer.encode_plus(sentence, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    #inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "    predicted_label = torch.argmax(outputs.logits).item()\n",
    "    predicted_label_name = list(numerize_labels.keys())[predicted_label]\n",
    "    result = pd.concat([result, pd.DataFrame({'ID': [i], 'Label': [predicted_label_name]})], ignore_index=True)\n",
    "\n",
    "result = result.iloc[:-1]\n",
    "result.to_csv(\"result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
